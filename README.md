# Sparkify Data Lake Project with Apache Spark
The purpose of this project is to build an ETL pipeline that will be able to extract song and log data from an S3 bucket, process the data using Spark and load the data back into s3 as a set of dimensional tables in spark parquet files. This helps analysts to continue finding insights on what their users are listening to.

## Database Schema Design
The tables created include one fact table, `songplays` and four dimensional tables namely `users`, `songs`, `artists` and `time`. This follows the star schema principle which will contain clean data that is suitable for OLAP(Online Analytical Processing) operations which will be what the analysts will need to conduct to find the insights they are looking for.

## ETL Pipeline
The data gets that gets extracted will need to be transformed to to fit the data model in the target destination tables. For instance the source data for timestamp is in unix format and that will need to be converted to timestamp from which the year, month, day, hour values etc can be extracted which will fit in the relevant target time and songplays table columns. The script will also need to cater for duplicates, ensuring that they aren't part of the final data that is loaded in the tables.

## Datasets used
Song Dataset<br>
The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.<br>

song_data/A/B/C/TRABCEI128F424C983.json<br>
song_data/A/A/B/TRAABJL12903CDCF1A.json<br>
And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.

`{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}`<br>
Log Dataset<br>
The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. <br>These simulate app activity logs from an imaginary music streaming app based on configuration settings.<br>

The log files in the dataset you'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.<br>

log_data/2018/11/2018-11-12-events.json<br>
log_data/2018/11/2018-11-13-events.json<br>

## Project Files
### etl.py
This script once executed retrieves the song and log data in the s3 bucket, transforms the data into fact and dimensional tables then loads the table data back into s3 as parquet files. 

### dl.cfg
Your AWS keys pairs.

## Getting Started
In order to have a copy of the project up and running locally, you will need to take note of the following:

### Prerequisites
   - Python 2.7 or greater.
   - AWS Account.

   - Set your AWS access and secret key in the `dl.clg` file. 
        ```
        [AWS]
        AWS_ACCESS_KEY_ID = <your aws key>
        AWS_SECRET_ACCESS_KEY = <your aws secret>
        ```
            
### Terminal commands
- Execute the ETL pipeline script by running:
    ```
    $ python etl.py
    ```

## Built With
- Python and pySpark


